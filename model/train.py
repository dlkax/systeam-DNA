"""
Sistema de Treinamento de CNN para Classifica√ß√£o de F√≥sseis
Autor: systeam-DNA
Vers√£o: 2.0
"""

import os
import json
import datetime
import tensorflow as tf
from tensorflow.keras.callbacks import (
    ModelCheckpoint, 
    EarlyStopping, 
    ReduceLROnPlateau,
    TensorBoard
)
import matplotlib.pyplot as plt
import numpy as np

# ===========================
# Configura√ß√µes
# ===========================
DATA_DIR = "data/images"
MODEL_DIR = "saved_model"
LABELS_FILE = "model/labels.json"
LOGS_DIR = "logs"

IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.001
VALIDATION_SPLIT = 0.2
TEST_SPLIT = 0.1

# ===========================
# Modelo - CORRIGIDO
# ===========================
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0

def create_model(num_classes: int, input_shape=(224, 224, 3), pretrained=True):
    """Cria o modelo CNN com EfficientNetB0 - VERS√ÉO CORRIGIDA"""
    
    if pretrained:
        weights = 'imagenet'
        # ‚ö†Ô∏è CORRE√á√ÉO: For√ßar input_shape para RGB quando usar pesos ImageNet
        input_shape = (224, 224, 3)
        print("üîß Usando pesos ImageNet - for√ßando input_shape=(224, 224, 3)")
    else:
        weights = None
        print(f"üîß Sem pesos pr√©-treinados - usando input_shape={input_shape}")

    try:
        # Backbone EfficientNetB0
        base_model = EfficientNetB0(
            include_top=False,
            weights=weights,
            input_shape=input_shape,
            pooling='avg'
        )
        
        print(f"‚úÖ EfficientNetB0 carregado com sucesso")
        print(f"üìê Input shape do modelo: {base_model.input_shape}")
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar EfficientNetB0: {str(e)}")
        print("üîÑ Tentando sem pesos pr√©-treinados...")
        
        # Fallback: criar modelo sem pesos pr√©-treinados
        base_model = EfficientNetB0(
            include_top=False,
            weights=None,
            input_shape=input_shape,
            pooling='avg'
        )
        pretrained = False

    # Congelar o backbone se usando pesos pr√©-treinados
    if pretrained:
        base_model.trainable = False
        print("üîí Backbone congelado para fine-tuning gradual")
    else:
        base_model.trainable = True
        print("üîì Backbone liberado para treinamento completo")

    # Construir o modelo completo
    model = models.Sequential([
        base_model,
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu', name='dense_1'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax', name='predictions')
    ], name='fossil_classifier')

    return model

# ===========================
# Classe de Treinamento
# ===========================
class FossilClassifierTrainer:
    def __init__(self):
        self.model = None
        self.history = None
        self.class_names = None
        self.config = {
            'img_size': IMG_SIZE,
            'batch_size': BATCH_SIZE,
            'epochs': EPOCHS,
            'learning_rate': LEARNING_RATE,
            'validation_split': VALIDATION_SPLIT,
            'test_split': TEST_SPLIT
        }

    # -------- Carregamento de Dados --------
    def load_and_prepare_data(self):
        print("üìÇ Carregando dataset...")
        
        # Verificar se o diret√≥rio existe
        if not os.path.exists(DATA_DIR):
            raise FileNotFoundError(f"Diret√≥rio de dados n√£o encontrado: {DATA_DIR}")

        try:
            train_ds = tf.keras.utils.image_dataset_from_directory(
                DATA_DIR,
                validation_split=VALIDATION_SPLIT,
                subset="training",
                seed=42,
                image_size=IMG_SIZE,
                batch_size=BATCH_SIZE,
                label_mode='categorical',
                color_mode='rgb'  # ‚ö†Ô∏è Garantir RGB
            )

            val_ds = tf.keras.utils.image_dataset_from_directory(
                DATA_DIR,
                validation_split=VALIDATION_SPLIT,
                subset="validation",
                seed=42,
                image_size=IMG_SIZE,
                batch_size=BATCH_SIZE,
                label_mode='categorical',
                color_mode='rgb'  # ‚ö†Ô∏è Garantir RGB
            )
        except Exception as e:
            print(f"‚ùå Erro ao carregar dataset: {str(e)}")
            raise

        self.class_names = train_ds.class_names
        print(f"‚úÖ Classes detectadas: {self.class_names}")
        print(f"üìä Total de classes: {len(self.class_names)}")
        
        # Verificar shape dos dados
        for images, labels in train_ds.take(1):
            print(f"üìê Shape das imagens: {images.shape}")
            print(f"üìä Shape dos labels: {labels.shape}")
            break
        
        self._save_class_mapping()

        # Augmenta√ß√£o de dados
        data_augmentation = tf.keras.Sequential([
            tf.keras.layers.RandomFlip("horizontal"),
            tf.keras.layers.RandomRotation(0.2),
            tf.keras.layers.RandomZoom(0.1),
            tf.keras.layers.RandomContrast(0.2),
        ], name='data_augmentation')

        train_ds = train_ds.map(
            lambda x, y: (data_augmentation(x, training=True), y),
            num_parallel_calls=tf.data.AUTOTUNE
        )

        # Normaliza√ß√£o
        normalization = tf.keras.layers.Rescaling(1./255, name='rescaling')
        train_ds = train_ds.map(lambda x, y: (normalization(x), y))
        val_ds = val_ds.map(lambda x, y: (normalization(x), y))

        # Otimiza√ß√µes de performance
        AUTOTUNE = tf.data.AUTOTUNE
        train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
        val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

        # Contar amostras
        self.train_samples = tf.data.experimental.cardinality(train_ds).numpy() * BATCH_SIZE
        self.val_samples = tf.data.experimental.cardinality(val_ds).numpy() * BATCH_SIZE

        print(f"üìà Amostras de treino: ~{self.train_samples}")
        print(f"üìâ Amostras de valida√ß√£o: ~{self.val_samples}")

        return train_ds, val_ds

    def _save_class_mapping(self):
        os.makedirs(os.path.dirname(LABELS_FILE), exist_ok=True)
        class_mapping = {
            "version": "2.0",
            "created_at": datetime.datetime.now().isoformat(),
            "num_classes": len(self.class_names),
            "id_to_name": {i: name for i, name in enumerate(self.class_names)},
            "name_to_id": {name: i for i, name in enumerate(self.class_names)},
            "config": self.config
        }
        with open(LABELS_FILE, "w", encoding='utf-8') as f:
            json.dump(class_mapping, f, indent=2, ensure_ascii=False)
        print(f"üíæ Mapeamento de classes salvo em: {LABELS_FILE}")

    # -------- Callbacks --------
    def create_callbacks(self):
        os.makedirs(MODEL_DIR, exist_ok=True)
        os.makedirs(LOGS_DIR, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        
        callbacks = [
            ModelCheckpoint(
                filepath=os.path.join(MODEL_DIR, "best_model.h5"),
                monitor='val_accuracy',
                mode='max',
                save_best_only=True,
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            TensorBoard(
                log_dir=os.path.join(LOGS_DIR, f"fit-{timestamp}"),
                histogram_freq=1,
                write_images=True
            )
        ]
        return callbacks

    # -------- Treinamento --------
    def train(self, train_ds, val_ds):
        print("\nüöÄ Iniciando treinamento...")
        
        try:
            # Criar modelo
            self.model = create_model(num_classes=len(self.class_names))
            
            # Compilar modelo
            self.model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                loss='categorical_crossentropy',
                metrics=[
                    'accuracy',
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall'),
                    tf.keras.metrics.AUC(name='auc')
                ]
            )
            
            print("\nüìê Arquitetura do modelo:")
            self.model.summary()
            
            # Criar callbacks
            callbacks = self.create_callbacks()
            
            # Treinar modelo
            self.history = self.model.fit(
                train_ds,
                validation_data=val_ds,
                epochs=EPOCHS,
                callbacks=callbacks,
                verbose=1
            )
            
            print("\n‚úÖ Treinamento conclu√≠do!")
            
        except Exception as e:
            print(f"‚ùå Erro durante o treinamento: {str(e)}")
            raise

    # -------- Avalia√ß√£o --------
    def evaluate_model(self, val_ds):
        print("\nüìä Avalia√ß√£o final do modelo:")
        try:
            results = self.model.evaluate(val_ds, verbose=1)
            metrics_names = self.model.metrics_names
            for name, value in zip(metrics_names, results):
                print(f"  {name}: {value:.4f}")
            return dict(zip(metrics_names, results))
        except Exception as e:
            print(f"‚ùå Erro na avalia√ß√£o: {str(e)}")
            return {}

    # -------- Plot History --------
    def plot_training_history(self):
        if not self.history:
            print("‚ö†Ô∏è Nenhum hist√≥rico de treinamento dispon√≠vel")
            return

        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))

            # Accuracy
            axes[0, 0].plot(self.history.history['accuracy'], label='Train')
            axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')
            axes[0, 0].set_title('Model Accuracy')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # Loss
            axes[0, 1].plot(self.history.history['loss'], label='Train')
            axes[0, 1].plot(self.history.history['val_loss'], label='Validation')
            axes[0, 1].set_title('Model Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # Precision
            if 'precision' in self.history.history:
                axes[1, 0].plot(self.history.history['precision'], label='Train')
                axes[1, 0].plot(self.history.history['val_precision'], label='Validation')
                axes[1, 0].set_title('Model Precision')
                axes[1, 0].legend()
                axes[1, 0].grid(True)

            # Recall
            if 'recall' in self.history.history:
                axes[1, 1].plot(self.history.history['recall'], label='Train')
                axes[1, 1].plot(self.history.history['val_recall'], label='Validation')
                axes[1, 1].set_title('Model Recall')
                axes[1, 1].legend()
                axes[1, 1].grid(True)

            plt.tight_layout()
            os.makedirs("plots", exist_ok=True)
            plt.savefig("plots/training_history.png", dpi=150, bbox_inches='tight')
            plt.show()
            print("üìà Gr√°ficos salvos em: plots/training_history.png")
            
        except Exception as e:
            print(f"‚ùå Erro ao plotar gr√°ficos: {str(e)}")

    # -------- Salvar Modelo --------
    def save_model(self):
        if not self.model:
            print("‚ö†Ô∏è Nenhum modelo para salvar")
            return
            
        try:
            os.makedirs(MODEL_DIR, exist_ok=True)
            
            # Salvar modelo completo
            model_path = os.path.join(MODEL_DIR, "fossil_classifier.h5")
            self.model.save(model_path)
            
            # Salvar apenas os pesos
            weights_path = os.path.join(MODEL_DIR, "model_weights.h5")
            self.model.save_weights(weights_path)
            
            # Salvar no formato TensorFlow
            tf_model_path = os.path.join(MODEL_DIR, "tf_saved_model")
            self.model.save(tf_model_path, save_format='tf')
            
            print(f"üíæ Modelos salvos em: {MODEL_DIR}")

            # Salvar metadados
            metadata = {
                "training_date": datetime.datetime.now().isoformat(),
                "tensorflow_version": tf.__version__,
                "epochs_trained": len(self.history.history['loss']) if self.history else 0,
                "final_metrics": {
                    "train_accuracy": float(self.history.history['accuracy'][-1]) if self.history else 0,
                    "val_accuracy": float(self.history.history['val_accuracy'][-1]) if self.history else 0,
                    "train_loss": float(self.history.history['loss'][-1]) if self.history else 0,
                    "val_loss": float(self.history.history['val_loss'][-1]) if self.history else 0,
                },
                "config": self.config,
                "classes": self.class_names,
                "model_input_shape": self.model.input_shape if self.model else None
            }
            
            metadata_path = os.path.join(MODEL_DIR, "training_metadata.json")
            with open(metadata_path, "w", encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            print(f"üìã Metadados salvos em: {metadata_path}")
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar modelo: {str(e)}")

# ===========================
# Main
# ===========================
def main():
    print("üñ•Ô∏è Configura√ß√£o do Sistema:")
    print(f"  TensorFlow vers√£o: {tf.__version__}")
    print(f"  Python vers√£o: {tf.version.VERSION}")
    
    # Configurar GPU se dispon√≠vel
    physical_devices = tf.config.list_physical_devices('GPU')
    print(f"  GPUs f√≠sicas dispon√≠veis: {len(physical_devices)}")
    
    if physical_devices:
        try:
            # Configurar crescimento de mem√≥ria
            for gpu in physical_devices:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"  ‚úÖ GPU configurada: {physical_devices[0].name}")
        except RuntimeError as e:
            print(f"  ‚ö†Ô∏è Erro na configura√ß√£o da GPU: {e}")
    else:
        print("  ‚ÑπÔ∏è Executando na CPU")
    
    # Verificar diret√≥rios
    print(f"\nüìÅ Verificando estrutura de diret√≥rios:")
    print(f"  Data dir: {DATA_DIR} - {'‚úÖ' if os.path.exists(DATA_DIR) else '‚ùå'}")
    
    trainer = FossilClassifierTrainer()

    try:
        # Pipeline de treinamento
        train_ds, val_ds = trainer.load_and_prepare_data()
        trainer.train(train_ds, val_ds)
        metrics = trainer.evaluate_model(val_ds)
        trainer.plot_training_history()
        trainer.save_model()
        
        print("\nüéâ Pipeline de treinamento conclu√≠do com sucesso!")
        if metrics.get('accuracy'):
            print(f"üìä Acur√°cia final de valida√ß√£o: {metrics.get('accuracy', 0):.2%}")
        print(f"\nüí° Para visualizar o TensorBoard:")
        print(f"    tensorboard --logdir {LOGS_DIR}")

    except Exception as e:
        print(f"\n‚ùå Erro durante o treinamento: {str(e)}")
        print("\nüîß Poss√≠veis solu√ß√µes:")
        print("1. Verifique se o diret√≥rio 'data/images' existe e cont√©m as imagens")
        print("2. Certifique-se de ter pelo menos 2 classes de imagens")
        print("3. Verifique se as imagens est√£o em formato v√°lido (jpg, png)")
        print("4. Tente executar sem pesos pr√©-treinados modificando pretrained=False")
        raise

if __name__ == "__main__":
    main()